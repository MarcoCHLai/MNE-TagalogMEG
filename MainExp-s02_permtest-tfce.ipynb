{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import mne, os, pickle, glob, datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats as stats\n",
    "from mne.stats import (spatio_temporal_cluster_test, f_mway_rm, f_threshold_mway_rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories\n",
    "mri_dir  = os.path.realpath('../Data/mri')     # mri directory\n",
    "meg_dir  = os.path.realpath('../Data/meg-exp') # meg directory\n",
    "log_dir  = os.path.join(meg_dir, 'log')        # log directory\n",
    "stc_dir  = os.path.join(meg_dir, 'stc')        # stc directory\n",
    "mod_dir  = os.path.join(meg_dir, 'mod')        # GLM model directory\n",
    "res_dir  = os.path.join(meg_dir, 'res')        # results directory\n",
    "\n",
    "# subject list\n",
    "subjects = [\n",
    "    'P049','P050','P054','P055','P056',\n",
    "    'P057','P058','P059','P060','P061',\n",
    "    'P062','P063','P064','P065','P068',\n",
    "    'P069','P070','P071'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define anatomical mask\n",
    "The anatomical mask is based on the meta-analysis in [Leminen et al. (2019) *Cortex*](https://doi.org/10.1016/j.cortex.2018.08.016) and includes the following brain regions from the Desikan-Killiany Atlas:\n",
    "- parsopercularis\n",
    "- parsorbitalis\n",
    "- parstriangularis\n",
    "- frontalpole\n",
    "- lateralorbitofrontal\n",
    "- medialorbitofrontal\n",
    "- superiortemporal\n",
    "- middletemporal\n",
    "- bankssts\n",
    "- transversetemporal\n",
    "- insula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading labels from parcellation...\n",
      "   read 35 labels from /Users/cl5564/Library/CloudStorage/Dropbox/OSF/TagalogViolation/Data/mri/fsaverage/label/lh.aparc.annot\n",
      "   read 34 labels from /Users/cl5564/Library/CloudStorage/Dropbox/OSF/TagalogViolation/Data/mri/fsaverage/label/rh.aparc.annot\n"
     ]
    }
   ],
   "source": [
    "# read labels\n",
    "labels = mne.read_labels_from_annot('fsaverage', 'aparc', 'both', subjects_dir=mri_dir)\n",
    "\n",
    "# define ROIs\n",
    "roi_list = ['parsopercularis', 'parsorbitalis', 'parstriangularis',\n",
    "            'frontalpole', 'lateralorbitofrontal', 'medialorbitofrontal',\n",
    "            'superiortemporal', 'middletemporal', 'bankssts', 'transversetemporal', 'insula']\n",
    "\n",
    "# loop over ROI list\n",
    "for roi_count, roi_name in enumerate(roi_list):\n",
    "    \n",
    "    # look for ROI info from the annotation labels\n",
    "    roi_lh_tmp = [label for label in labels if label.name == f'{roi_name}-lh'][0]\n",
    "    roi_rh_tmp = [label for label in labels if label.name == f'{roi_name}-rh'][0]\n",
    "    \n",
    "    # combine ROIs\n",
    "    if roi_count == 0:\n",
    "        roi_lh = roi_lh_tmp\n",
    "        roi_rh = roi_rh_tmp\n",
    "    else:\n",
    "        roi_lh += roi_lh_tmp\n",
    "        roi_rh += roi_rh_tmp\n",
    "\n",
    "# set up vertex indices\n",
    "n_hemisources = 2562  # number of vertices in each hemisphere\n",
    "hemi_idx = np.arange(0, n_hemisources, 1)  # create an array\n",
    "\n",
    "# look for ROI vertex indices in the LH, RH, and both himispheres\n",
    "roi_idx_lh = roi_lh.get_vertices_used(vertices=hemi_idx)\n",
    "roi_idx_rh = roi_rh.get_vertices_used(vertices=hemi_idx)\n",
    "roi_idx_bh = np.concatenate((roi_idx_lh, roi_idx_rh+n_hemisources))\n",
    "\n",
    "# look for ROI vertex indices NOT in the LH, RH, and both himispheres\n",
    "diff_idx_lh = np.setdiff1d(hemi_idx, roi_idx_lh)\n",
    "diff_idx_rh = np.setdiff1d(hemi_idx, roi_idx_rh)\n",
    "diff_idx_bh = np.concatenate((diff_idx_lh, diff_idx_rh+n_hemisources))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load beta map\n",
    "- Model: 'constant', 'ArgStrViolNAG', 'ArgStrViolNA', 'CatViolNAG', 'CatViolNA', 'GrammNAG', 'GrammNA', 'Filler', 'LogBaseFrequency', 'WordLength', 'RespACC', 'trial order'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P049 P050 P054 P055 P056 P057 P058 P059 P060 P061 P062 P063 P064 P065 P068 P069 P070 P071 \n",
      "(18, 5124, 701, 7)\n"
     ]
    }
   ],
   "source": [
    "# define event codes\n",
    "event_id  = dict(ArgStrViolNAG = 10, ArgStrViolNA = 20, \n",
    "                 CatViolNAG    = 30, CatViolNA    = 40, \n",
    "                 GrammNAG      = 50, GrammNA      = 60, \n",
    "                 Filler        = 70)\n",
    "cond_code = list(event_id.values())  # condition code\n",
    "cond_name = list(event_id.keys())    # condition name\n",
    "\n",
    "# define regressors (12 regressors)\n",
    "# constant + 7 conditions + LogBaseFrequency + WordLength + accuracy + trial order\n",
    "col_labels = cond_name.copy()\n",
    "col_labels.insert(0, 'constant')\n",
    "for item in ['LogBaseFreq', 'Length', 'RespACC', 'trial order']: col_labels.append(item)\n",
    "\n",
    "# define variables\n",
    "epoch_tmin = -0.1  # epoch onset\n",
    "epoch_tmax = 0.6   # epoch offset\n",
    "times      = np.arange(epoch_tmin*1000, epoch_tmax*1000+1, 1)\n",
    "n_subj     = len(subjects)    # number of subjects\n",
    "n_cond     = len(cond_code)   # number of conditions\n",
    "n_reg      = len(col_labels)  # number of regressors\n",
    "n_times    = len(times)       # numner of time points\n",
    "n_sources  = n_hemisources*2  # number of sources\n",
    "\n",
    "# create an empty matrix for storing beta map\n",
    "data_mtx  = np.empty((n_subj, n_sources, n_times, n_reg))\n",
    "\n",
    "# load beta map\n",
    "for s, subj in enumerate(subjects):\n",
    "    print(subj, end=' ')\n",
    "    fname    = os.path.join(mod_dir, '%s_reg%s_b-map.npy') %(subj, n_reg)\n",
    "    data_val = np.load(fname)\n",
    "    data_mtx[s,:,:,:] = data_val\n",
    "\n",
    "data_mtx = data_mtx[:,:,:,1:n_cond+1] # keep condition regressors\n",
    "print();print(data_mtx.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define stats function of rmANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_fun(*args):\n",
    "    # get f-values only.\n",
    "    return f_mway_rm(np.swapaxes(args, 1, 0), factor_levels=factor_levels,\n",
    "                     effects=effects, return_pvals=return_pvals)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## licensing stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell arrangement: A1B1 A1B2 A2B1 A2B2\n",
    "# 'A': main effect of A (violation type)\n",
    "# 'B': main effect of B (prefix type)\n",
    "# 'A:B': interaction effect\n",
    "\n",
    "factor_levels = [2, 2]\n",
    "effects       = 'A:B'\n",
    "return_pvals  = False # Tell the ANOVA not to compute p-values which we don't need for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 18, 151, 5124)\n",
      "(4, 18, 151, 2562)\n",
      "(4, 18, 151, 2562)\n"
     ]
    }
   ],
   "source": [
    "# define time of interest\n",
    "toi_min = 150\n",
    "toi_max = 300\n",
    "toi     = np.arange(toi_min, toi_max+1, 1)\n",
    "toi_min_idx = np.squeeze(np.where(times==toi_min))\n",
    "toi_max_idx = np.squeeze(np.where(times==toi_max))\n",
    "toi_idx     = np.arange(toi_min_idx,toi_max_idx+1)\n",
    "toi_ntimes  = toi_idx.size\n",
    "\n",
    "# prep for rmANOVA\n",
    "n_cell  = np.prod(factor_levels) # number of cells\n",
    "data_bh = [] # create an empty list for storing data of both hemispheres\n",
    "data_lh = [] # create an empty list for storing data of LH\n",
    "data_rh = [] # create an empty list for storing data of RH\n",
    "\n",
    "# retrieve data for the conditions of interest\n",
    "data    = data_mtx[:,:,:,1:n_cell+1]\n",
    "\n",
    "for i in range(n_cell):\n",
    "    \n",
    "    data_temp = data[:,:,toi_idx][:,:,:,i]         # select data based on time of interest\n",
    "    data_temp = np.transpose(data_temp, [0, 2, 1]) # transpose the matrix into: subjects x times x sources \n",
    "    data_bh.append(data_temp)\n",
    "    data_lh.append(data_temp[:,:,:n_hemisources])\n",
    "    data_rh.append(data_temp[:,:,n_hemisources:])\n",
    "\n",
    "print(np.shape(data_bh))\n",
    "print(np.shape(data_lh))\n",
    "print(np.shape(data_rh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Reading a source space...\n",
      "    Computing patch statistics...\n",
      "    Patch information added...\n",
      "    Distance information added...\n",
      "    [done]\n",
      "    Reading a source space...\n",
      "    Computing patch statistics...\n",
      "    Patch information added...\n",
      "    Distance information added...\n",
      "    [done]\n",
      "    2 source spaces read\n",
      "-- number of adjacent vertices : 2562\n",
      "permutation test...\n",
      "stat_fun(H1): min=0.000000 max=45.464814\n",
      "Running initial clustering …\n",
      "Using 405 thresholds from 5.00 to 45.40 for TFCE computation (h_power=2.00, e_power=0.50)\n",
      "Found 386862 clusters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2b210f344e424bab22fb8f5ab56e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Permuting : 0/9999 [00:00<?,       ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define parameters\n",
    "hemi            = 'lh'\n",
    "X               = eval('data_%s'%hemi)\n",
    "spatial_exclude = eval('diff_idx_%s'%hemi)\n",
    "tail            = 1 # F test: use the upper tail (see also: https://stats.stackexchange.com/a/73993)\n",
    "p_thresh        = 0.05\n",
    "n_permutations  = 10000\n",
    "df              = n_subj-1\n",
    "f_thresh        = f_threshold_mway_rm(n_subj, factor_levels, effects, p_thresh)\n",
    "\n",
    "# read source space & compute adjacency\n",
    "src_fname = os.path.join(mri_dir, 'fsaverage', 'bem', 'fsaverage-ico-4-src.fif')\n",
    "src = mne.read_source_spaces(src_fname)\n",
    "if hemi == 'lh':\n",
    "    adjacency = mne.spatial_src_adjacency(src[:1]) \n",
    "elif hemi == 'rh':\n",
    "    adjacency = mne.spatial_src_adjacency(src[1:])\n",
    "elif hemi == 'bh':\n",
    "    adjacency = mne.spatial_src_adjacency(src)\n",
    "\n",
    "# permutation test with TFCE\n",
    "print('permutation test...')\n",
    "threshold_tfce = dict(start=np.ceil(f_thresh), step=0.1)\n",
    "f_tfce, clusters, p_tfce, H0 = clu = \\\n",
    "    spatio_temporal_cluster_test(X,\n",
    "                                 tail            = tail,\n",
    "                                 threshold       = threshold_tfce,\n",
    "                                 stat_fun        = stat_fun,\n",
    "                                 n_permutations  = n_permutations,\n",
    "                                 adjacency       = adjacency,\n",
    "                                 spatial_exclude = spatial_exclude,\n",
    "                                 n_jobs          = 10,\n",
    "                                 seed            = 1119,\n",
    "                                 buffer_size     = None,\n",
    "                                 verbose         = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time range: 175-217ms\n",
      "LH vertices: 126\n"
     ]
    }
   ],
   "source": [
    "p_thresh = 0.05\n",
    "pval     = np.reshape(p_tfce, [toi_ntimes, n_hemisources])\n",
    "pval_cmp = pval <= p_thresh\n",
    "    \n",
    "vidx = np.unique(np.where(pval_cmp)[1])\n",
    "tidx = np.unique(np.where(pval_cmp)[0])\n",
    "clu_tmin  = toi[tidx[0]]\n",
    "clu_tmax  = toi[tidx[-1]]\n",
    "clu_times = np.arange(clu_tmin, clu_tmax+1)\n",
    "\n",
    "if clu_times.size == tidx.size:\n",
    "    print('time range: %s-%sms' %(clu_tmin, clu_tmax))\n",
    "    print('LH vertices: %s' %(vidx.size))\n",
    "else:\n",
    "    print('time range is not continuous...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "if effects == 'A':\n",
    "    effect_name = 'mainViol'\n",
    "elif effects == 'B':\n",
    "    effect_name = 'mainPrefix'\n",
    "elif effects == 'A:B':\n",
    "    effect_name = 'Interaction'\n",
    "\n",
    "current_date  = datetime.date.today()\n",
    "contrast_name = 'reg%s_%s-%s' %(n_reg, effect_name, hemi)\n",
    "pickle_fname  = os.path.join(res_dir, 'res_%s_%s-%s_%ssubjs_tfce_%s.pickled') % (contrast_name, toi_min, toi_max, n_subj, current_date)\n",
    "open_file     = open(pickle_fname, \"wb\")\n",
    "pickle.dump(clu, open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## composition stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell arrangement: A1B1 A1B2 A2B1 A2B2\n",
    "# 'A': main effect of A (violation type)\n",
    "# 'B': main effect of B (prefix type)\n",
    "# 'A:B': interaction effect\n",
    "\n",
    "factor_levels = [2, 2]\n",
    "effects       = 'A'\n",
    "return_pvals  = False # Tell the ANOVA not to compute p-values which we don't need for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 18, 151, 5124)\n",
      "(4, 18, 151, 2562)\n",
      "(4, 18, 151, 2562)\n"
     ]
    }
   ],
   "source": [
    "# define time of interest\n",
    "toi_min = 300\n",
    "toi_max = 450\n",
    "toi     = np.arange(toi_min, toi_max+1, 1)\n",
    "toi_min_idx = np.squeeze(np.where(times==toi_min))\n",
    "toi_max_idx = np.squeeze(np.where(times==toi_max))\n",
    "toi_idx     = np.arange(toi_min_idx,toi_max_idx+1)\n",
    "toi_ntimes  = toi_idx.size\n",
    "\n",
    "# prep for rmANOVA\n",
    "n_cell  = np.prod(factor_levels) # number of cells\n",
    "data_bh = [] # create an empty list for storing data of both hemispheres\n",
    "data_lh = [] # create an empty list for storing data of LH\n",
    "data_rh = [] # create an empty list for storing data of RH\n",
    "\n",
    "# retrieve data for the conditions of interest\n",
    "data    = data_mtx[:,:,:,1:n_cell+1]\n",
    "\n",
    "for i in range(n_cell):\n",
    "    \n",
    "    data_temp = data[:,:,toi_idx][:,:,:,i]         # select data based on time of interest\n",
    "    data_temp = np.transpose(data_temp, [0, 2, 1]) # transpose the matrix into: subjects x times x sources \n",
    "    data_bh.append(data_temp)\n",
    "    data_lh.append(data_temp[:,:,:n_hemisources])\n",
    "    data_rh.append(data_temp[:,:,n_hemisources:])\n",
    "\n",
    "print(np.shape(data_bh))\n",
    "print(np.shape(data_lh))\n",
    "print(np.shape(data_rh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Reading a source space...\n",
      "    Computing patch statistics...\n",
      "    Patch information added...\n",
      "    Distance information added...\n",
      "    [done]\n",
      "    Reading a source space...\n",
      "    Computing patch statistics...\n",
      "    Patch information added...\n",
      "    Distance information added...\n",
      "    [done]\n",
      "    2 source spaces read\n",
      "-- number of adjacent vertices : 2562\n",
      "permutation test...\n",
      "stat_fun(H1): min=0.000000 max=45.049390\n",
      "Running initial clustering …\n",
      "Using 401 thresholds from 5.00 to 45.00 for TFCE computation (h_power=2.00, e_power=0.50)\n",
      "Found 386862 clusters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f6abc556e141bb94973a349632c017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Permuting : 0/9999 [00:00<?,       ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define parameters\n",
    "hemi            = 'lh'\n",
    "X               = eval('data_%s'%hemi)\n",
    "spatial_exclude = eval('diff_idx_%s'%hemi)\n",
    "tail            = 1 # F test: use the upper tail (see also: https://stats.stackexchange.com/a/73993)\n",
    "p_thresh        = 0.05\n",
    "n_permutations  = 10000\n",
    "df              = n_subj-1\n",
    "f_thresh        = f_threshold_mway_rm(n_subj, factor_levels, effects, p_thresh)\n",
    "\n",
    "# read source space & compute adjacency\n",
    "src_fname = os.path.join(mri_dir, 'fsaverage', 'bem', 'fsaverage-ico-4-src.fif')\n",
    "src = mne.read_source_spaces(src_fname)\n",
    "if hemi == 'lh':\n",
    "    adjacency = mne.spatial_src_adjacency(src[:1]) \n",
    "elif hemi == 'rh':\n",
    "    adjacency = mne.spatial_src_adjacency(src[1:])\n",
    "elif hemi == 'bh':\n",
    "    adjacency = mne.spatial_src_adjacency(src)\n",
    "\n",
    "# permutation test with TFCE\n",
    "print('permutation test...')\n",
    "threshold_tfce = dict(start=np.ceil(f_thresh), step=0.1)\n",
    "f_tfce, clusters, p_tfce, H0 = clu = \\\n",
    "    spatio_temporal_cluster_test(X,\n",
    "                                 tail            = tail,\n",
    "                                 threshold       = threshold_tfce,\n",
    "                                 stat_fun        = stat_fun,\n",
    "                                 n_permutations  = n_permutations,\n",
    "                                 adjacency       = adjacency,\n",
    "                                 spatial_exclude = spatial_exclude,\n",
    "                                 n_jobs          = 10,\n",
    "                                 seed            = 1119,\n",
    "                                 buffer_size     = None,\n",
    "                                 verbose         = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time range: 361-398ms\n",
      "LH vertices: 95\n"
     ]
    }
   ],
   "source": [
    "p_thresh = 0.05\n",
    "pval     = np.reshape(p_tfce, [toi_ntimes, n_hemisources])\n",
    "pval_cmp = pval <= p_thresh\n",
    "    \n",
    "vidx = np.unique(np.where(pval_cmp)[1])\n",
    "tidx = np.unique(np.where(pval_cmp)[0])\n",
    "clu_tmin  = toi[tidx[0]]\n",
    "clu_tmax  = toi[tidx[-1]]\n",
    "clu_times = np.arange(clu_tmin, clu_tmax+1)\n",
    "\n",
    "if clu_times.size == tidx.size:\n",
    "    print('time range: %s-%sms' %(clu_tmin, clu_tmax))\n",
    "    print('LH vertices: %s' %(vidx.size))\n",
    "else:\n",
    "    print('time range is not continuous...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "if effects == 'A':\n",
    "    effect_name = 'mainViol'\n",
    "elif effects == 'B':\n",
    "    effect_name = 'mainPrefix'\n",
    "elif effects == 'A:B':\n",
    "    effect_name = 'Interaction'\n",
    "\n",
    "current_date  = datetime.date.today()\n",
    "contrast_name = 'reg%s_%s-%s' %(n_reg, effect_name, hemi)\n",
    "pickle_fname  = os.path.join(res_dir, 'res_%s_%s-%s_%ssubjs_tfce_%s.pickled') % (contrast_name, toi_min, toi_max, n_subj, current_date)\n",
    "open_file     = open(pickle_fname, \"wb\")\n",
    "pickle.dump(clu, open_file)\n",
    "open_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
