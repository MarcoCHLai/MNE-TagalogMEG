{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import mne, os, pickle, glob, datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats as stats\n",
    "from mne.stats import spatio_temporal_cluster_1samp_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories\n",
    "mri_dir  = os.path.realpath('../Data/mri')           # mri directory\n",
    "meg_dir  = os.path.realpath('../Data/meg-localizer') # meg directory\n",
    "log_dir  = os.path.join(meg_dir, 'log')              # log directory\n",
    "stc_dir  = os.path.join(meg_dir, 'stc')              # stc directory\n",
    "mod_dir  = os.path.join(meg_dir, 'mod')              # GLM model directory\n",
    "res_dir  = os.path.join(meg_dir, 'res')              # results directory\n",
    "\n",
    "# subject list\n",
    "subjects = [\n",
    "    'P049','P050','P054','P056','P057',\n",
    "    'P058','P060','P061','P062','P063',\n",
    "    'P064','P065','P066','P068','P069',\n",
    "    'P070','P071'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define anatomical mask: bilateral ventral temporo-occipital regions\n",
    "Following the approach in [Gwilliam et al. (2016) *NeuroImage*](https://doi.org/10.1016/j.neuroimage.2016.02.057), the anatomical mask includes the following brain regions from the Desikan-Killiany Atlas:\n",
    "- lateraloccipital\n",
    "- cuneus\n",
    "- lingual\n",
    "- pericalcarine\n",
    "- fusiform\n",
    "- middle-temporal\n",
    "- inferior-temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading labels from parcellation...\n",
      "   read 35 labels from /Users/cl5564/Library/CloudStorage/Dropbox/OSF/TagalogViolation/Data/mri/fsaverage/label/lh.aparc.annot\n",
      "   read 34 labels from /Users/cl5564/Library/CloudStorage/Dropbox/OSF/TagalogViolation/Data/mri/fsaverage/label/rh.aparc.annot\n"
     ]
    }
   ],
   "source": [
    "# read labels\n",
    "labels = mne.read_labels_from_annot('fsaverage', 'aparc', 'both', subjects_dir=mri_dir)\n",
    "\n",
    "# define ROIs\n",
    "roi_list = ['lateraloccipital', 'cuneus', 'lingual', 'pericalcarine', \n",
    "            'fusiform', 'middletemporal', 'inferiortemporal']\n",
    "\n",
    "# loop over ROI list\n",
    "for roi_count, roi_name in enumerate(roi_list):\n",
    "    \n",
    "    # look for ROI info from the annotation labels \n",
    "    roi_lh_tmp = [label for label in labels if label.name == f'{roi_name}-lh'][0]\n",
    "    roi_rh_tmp = [label for label in labels if label.name == f'{roi_name}-rh'][0]\n",
    "    \n",
    "    # combine ROIs\n",
    "    if roi_count == 0:\n",
    "        roi_lh = roi_lh_tmp\n",
    "        roi_rh = roi_rh_tmp\n",
    "    else:\n",
    "        roi_lh += roi_lh_tmp\n",
    "        roi_rh += roi_rh_tmp\n",
    "\n",
    "# set up vertex indices\n",
    "n_hemisources = 2562  # number of vertices in each hemisphere\n",
    "hemi_idx = np.arange(0, n_hemisources, 1)  # create an array\n",
    "\n",
    "# look for ROI vertex indices in the LH, RH, and both himispheres\n",
    "roi_idx_lh = roi_lh.get_vertices_used(vertices=hemi_idx)\n",
    "roi_idx_rh = roi_rh.get_vertices_used(vertices=hemi_idx)\n",
    "roi_idx_bh = np.concatenate((roi_idx_lh, roi_idx_rh+n_hemisources))\n",
    "\n",
    "# look for ROI vertex indices NOT in the LH, RH, and both himispheres\n",
    "diff_idx_lh = np.setdiff1d(hemi_idx, roi_idx_lh)\n",
    "diff_idx_rh = np.setdiff1d(hemi_idx, roi_idx_rh)\n",
    "diff_idx_bh = np.concatenate((diff_idx_lh, diff_idx_rh+n_hemisources))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load beta map\n",
    "- Model: 'constant', 'word_clean', 'word_noisy', 'word_symbol', 'letter_clean', 'letter_noisy', 'letter_symbol', 'trial order'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P049 P050 P054 P056 P057 P058 P060 P061 P062 P063 P064 P065 P066 P068 P069 P070 P071 \n",
      "(17, 5124, 701, 6)\n"
     ]
    }
   ],
   "source": [
    "# define event codes\n",
    "event_id  = dict(word_clean   = 1, word_noisy   = 2, word_symbol  = 4,\n",
    "                 letter_clean = 8, letter_noisy = 9, letter_symbol = 3)\n",
    "cond_code = list(event_id.values())  # condition code\n",
    "cond_name = list(event_id.keys())    # condition name\n",
    "\n",
    "# define regressors (8 regressors)\n",
    "# constant + 6 conditions + trial order\n",
    "col_labels = cond_name.copy()\n",
    "col_labels.insert(0, 'constant')\n",
    "for item in ['trial order']: col_labels.append(item)\n",
    "\n",
    "# define variables\n",
    "epoch_tmin = -0.1  # epoch onset\n",
    "epoch_tmax = 0.6   # epoch offset\n",
    "times      = np.arange(epoch_tmin*1000, epoch_tmax*1000+1, 1)\n",
    "n_subj     = len(subjects)    # number of subjects\n",
    "n_cond     = len(cond_code)   # number of conditions\n",
    "n_reg      = len(col_labels)  # number of regressors\n",
    "n_times    = len(times)       # numner of time points\n",
    "n_sources  = n_hemisources*2  # number of sources\n",
    "\n",
    "# create an empty matrix for storing beta map\n",
    "data_mtx  = np.empty((n_subj, n_sources, n_times, n_reg))\n",
    "\n",
    "# load beta map\n",
    "for s, subj in enumerate(subjects):\n",
    "    print(subj, end=' ')\n",
    "    fname    = os.path.join(mod_dir, '%s_reg%s_b-map.npy') %(subj, n_reg)\n",
    "    data_val = np.load(fname)\n",
    "    data_mtx[s,:,:,:] = data_val\n",
    "\n",
    "data_mtx = data_mtx[:,:,:,1:7] # keep condition regressors\n",
    "print();print(data_mtx.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String effect\n",
    "- contrast: 4-unit word > 4-unit symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define time of interest\n",
    "toi_min = 130\n",
    "toi_max = 180\n",
    "toi     = np.arange(toi_min, toi_max+1, 1)\n",
    "toi_min_idx = np.squeeze(np.where(times==toi_min))\n",
    "toi_max_idx = np.squeeze(np.where(times==toi_max))\n",
    "toi_idx     = np.arange(toi_min_idx,toi_max_idx+1)\n",
    "toi_ntimes  = toi_idx.size\n",
    "\n",
    "# conditions: word_clean, word_noisy, word_symbol, letter_clean, letter_noisy, letter_symbol\n",
    "# contrast: 4-unit word > 4-unit symbol\n",
    "contrast = [1, 0, -1, 0, 0, 0]\n",
    "data = np.dot(data_mtx, contrast)\n",
    "data = np.transpose(data, [0, 2, 1]) # transpose the matrix into: subjects x times x sources\n",
    "data_bh = data[:,toi_idx,:]\n",
    "data_lh = data[:,toi_idx,:n_hemisources]\n",
    "data_rh = data[:,toi_idx,n_hemisources:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Reading a source space...\n",
      "    Computing patch statistics...\n",
      "    Patch information added...\n",
      "    Distance information added...\n",
      "    [done]\n",
      "    Reading a source space...\n",
      "    Computing patch statistics...\n",
      "    Patch information added...\n",
      "    Distance information added...\n",
      "    [done]\n",
      "    2 source spaces read\n",
      "-- number of adjacent vertices : 5124\n",
      "permutation test...\n",
      "stat_fun(H1): min=-5.994030 max=6.725490\n",
      "Running initial clustering â€¦\n",
      "Using 48 thresholds from 2.00 to 6.70 for TFCE computation (h_power=2.00, e_power=0.50)\n",
      "Found 261324 clusters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f726af80ec347d7b1a8dd27b2efe27c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Permuting : 0/9999 [00:00<?,       ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define parameters\n",
    "hemi            = 'bh'\n",
    "X               = eval('data_%s'%hemi)\n",
    "spatial_exclude = eval('diff_idx_%s'%hemi)\n",
    "tail            = 1\n",
    "p_thresh        = 0.05\n",
    "n_permutations  = 10000\n",
    "df              = n_subj-1\n",
    "\n",
    "# read source space & compute adjacency\n",
    "src_fname = os.path.join(mri_dir, 'fsaverage', 'bem', 'fsaverage-ico-4-src.fif')\n",
    "src = mne.read_source_spaces(src_fname)\n",
    "if hemi == 'bh':\n",
    "    adjacency = mne.spatial_src_adjacency(src)\n",
    "elif hemi == 'lh':\n",
    "    adjacency = mne.spatial_src_adjacency(src[:1])\n",
    "elif hemi == 'rh':\n",
    "    adjacency = mne.spatial_src_adjacency(src[1:])\n",
    "\n",
    "# define threshold\n",
    "if tail == -1:\n",
    "    t_thresh = -stats.distributions.t.ppf(1-p_thresh, df=df)\n",
    "    threshold_tfce = dict(start=np.round(t_thresh), step=0.1*tail)\n",
    "else:\n",
    "    t_thresh = stats.distributions.t.ppf(1-p_thresh, df=df)\n",
    "    threshold_tfce = dict(start=np.round(t_thresh), step=0.1)\n",
    "\n",
    "# permutation test with TFCE\n",
    "print('permutation test...')\n",
    "t_tfce, clusters, p_tfce, H0 = clu = \\\n",
    "    spatio_temporal_cluster_1samp_test(X,\n",
    "                                       tail            = tail,\n",
    "                                       threshold       = threshold_tfce,\n",
    "                                       n_permutations  = n_permutations,\n",
    "                                       adjacency       = adjacency, \n",
    "                                       spatial_exclude = spatial_exclude,\n",
    "                                       n_jobs          = 10,\n",
    "                                       seed            = 1119,\n",
    "                                       buffer_size     = None,\n",
    "                                       verbose         = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time range: 130-170ms\n",
      "LH vertices: 61\n",
      "RH vertices: 0\n"
     ]
    }
   ],
   "source": [
    "p_thresh = 0.05\n",
    "pval = np.reshape(p_tfce, [toi_ntimes, n_sources])\n",
    "if tail == -1:\n",
    "    pval_cmp = pval > 1-p_thresh\n",
    "else:\n",
    "    pval_cmp = pval < p_thresh\n",
    "    \n",
    "vidx = np.unique(np.where(pval_cmp)[1])\n",
    "tidx = np.unique(np.where(pval_cmp)[0])\n",
    "clu_tmin  = toi[tidx[0]]\n",
    "clu_tmax  = toi[tidx[-1]]\n",
    "clu_times = np.arange(clu_tmin, clu_tmax+1)\n",
    "\n",
    "if clu_times.size == tidx.size:\n",
    "    print('time range: %s-%sms' %(clu_tmin, clu_tmax))\n",
    "    print('LH vertices: %s' %(sum(vidx < n_hemisources)))\n",
    "    print('RH vertices: %s' %(sum(vidx >= n_hemisources)))\n",
    "else:\n",
    "    print('time range is not continuous...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save permutation test results\n",
    "current_date  = datetime.date.today()\n",
    "contrast_name = 'reg%s_string' %(n_reg)\n",
    "pickle_fname  = os.path.join(res_dir, 'res_%s_%s-%s_%ssubjs_tfce_%s.pickled') % (contrast_name, toi_min, toi_max, n_subj, current_date)\n",
    "open_file     = open(pickle_fname, 'wb')\n",
    "pickle.dump(clu, open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise effect\n",
    "- contrast: (noisy word + noisy letter) > (clean word + clean letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orgaize data\n",
    "toi_min = 80\n",
    "toi_max = 130\n",
    "toi     = np.arange(toi_min, toi_max+1, 1)\n",
    "toi_min_idx = np.squeeze(np.where(times==toi_min))\n",
    "toi_max_idx = np.squeeze(np.where(times==toi_max))\n",
    "toi_idx     = np.arange(toi_min_idx,toi_max_idx+1)\n",
    "toi_ntimes  = toi_idx.size\n",
    "\n",
    "# conditions: word_clean, word_noisy, word_symbol, letter_clean, letter_noisy, letter_symbol\n",
    "# contrast: (noisy word + noisy letter) > (clean word + clean letter)\n",
    "contrast = [-0.5, 0.5, 0, -0.5, 0.5, 0]\n",
    "data = np.dot(data_mtx, contrast)\n",
    "data = np.transpose(data, [0, 2, 1]) # transpose the matrix into: subjects x times x sources\n",
    "data_bh = data[:,toi_idx,:]\n",
    "data_lh = data[:,toi_idx,:n_hemisources]\n",
    "data_rh = data[:,toi_idx,n_hemisources:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Reading a source space...\n",
      "    Computing patch statistics...\n",
      "    Patch information added...\n",
      "    Distance information added...\n",
      "    [done]\n",
      "    Reading a source space...\n",
      "    Computing patch statistics...\n",
      "    Patch information added...\n",
      "    Distance information added...\n",
      "    [done]\n",
      "    2 source spaces read\n",
      "-- number of adjacent vertices : 5124\n",
      "permutation test...\n",
      "stat_fun(H1): min=-6.526754 max=6.221838\n",
      "Running initial clustering â€¦\n",
      "Using 46 thresholds from -2.00 to -6.50 for TFCE computation (h_power=2.00, e_power=0.50)\n",
      "Found 261324 clusters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ec4d05651948efb3b396156eade3b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | Permuting : 0/9999 [00:00<?,       ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define parameters\n",
    "hemi            = 'bh'\n",
    "X               = eval('data_%s'%hemi)\n",
    "spatial_exclude = eval('diff_idx_%s'%hemi)\n",
    "tail            = -1\n",
    "p_thresh        = 0.05\n",
    "n_permutations  = 10000\n",
    "df              = n_subj-1\n",
    "\n",
    "# read source space & compute adjacency\n",
    "src_fname = os.path.join(mri_dir, 'fsaverage', 'bem', 'fsaverage-ico-4-src.fif')\n",
    "src = mne.read_source_spaces(src_fname)\n",
    "if hemi == 'bh':\n",
    "    adjacency = mne.spatial_src_adjacency(src)\n",
    "elif hemi == 'lh':\n",
    "    adjacency = mne.spatial_src_adjacency(src[:1])\n",
    "elif hemi == 'rh':\n",
    "    adjacency = mne.spatial_src_adjacency(src[1:])\n",
    "\n",
    "# define threshold\n",
    "if tail == -1:\n",
    "    t_thresh = -stats.distributions.t.ppf(1-p_thresh, df=df)\n",
    "    threshold_tfce = dict(start=np.round(t_thresh), step=0.1*tail)\n",
    "else:\n",
    "    t_thresh = stats.distributions.t.ppf(1-p_thresh, df=df)\n",
    "    threshold_tfce = dict(start=np.round(t_thresh), step=0.1)\n",
    "\n",
    "# permutation test with TFCE\n",
    "print('permutation test...')\n",
    "t_tfce, clusters, p_tfce, H0 = clu = \\\n",
    "    spatio_temporal_cluster_1samp_test(X,\n",
    "                                       tail            = tail,\n",
    "                                       threshold       = threshold_tfce,\n",
    "                                       n_permutations  = n_permutations,\n",
    "                                       adjacency       = adjacency, \n",
    "                                       spatial_exclude = spatial_exclude,\n",
    "                                       n_jobs          = 10,\n",
    "                                       seed            = 1119,\n",
    "                                       buffer_size     = None,\n",
    "                                       verbose         = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time range: 80-130ms\n",
      "LH vertices: 87\n",
      "RH vertices: 78\n"
     ]
    }
   ],
   "source": [
    "p_thresh = 0.05\n",
    "pval = np.reshape(p_tfce, [toi_ntimes, n_sources])\n",
    "if tail == -1:\n",
    "    pval_cmp = pval > 1-p_thresh\n",
    "else:\n",
    "    pval_cmp = pval < p_thresh\n",
    "    \n",
    "vidx = np.unique(np.where(pval_cmp)[1])\n",
    "tidx = np.unique(np.where(pval_cmp)[0])\n",
    "clu_tmin  = toi[tidx[0]]\n",
    "clu_tmax  = toi[tidx[-1]]\n",
    "clu_times = np.arange(clu_tmin, clu_tmax+1)\n",
    "\n",
    "if clu_times.size == tidx.size:\n",
    "    print('time range: %s-%sms' %(clu_tmin, clu_tmax))\n",
    "    print('LH vertices: %s' %(sum(vidx < n_hemisources)))\n",
    "    print('RH vertices: %s' %(sum(vidx >= n_hemisources)))\n",
    "else:\n",
    "    print('time range is not continuous...') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save permutation test results\n",
    "current_date  = datetime.date.today()\n",
    "contrast_name = 'reg%s_noise' %(n_reg)\n",
    "pickle_fname  = os.path.join(res_dir, 'res_%s_%s-%s_%ssubjs_tfce_%s.pickled') % (contrast_name, toi_min, toi_max, n_subj, current_date)\n",
    "open_file     = open(pickle_fname, 'wb')\n",
    "pickle.dump(clu, open_file)\n",
    "open_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
